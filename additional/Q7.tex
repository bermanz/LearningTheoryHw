\subsubsection{}
Suppose $P(y=-1) \leq \frac{1}{2} + \frac{1}{2k}$. We'll begin by using total probability, that is:
\begin{equation*}
    \begin{split}  
        P(sgn(w \cdot x) = y) = P(y=-1)P(sgn(w \cdot x) = y | y=-1) + P(y=1)P(sgn(w \cdot x) = y | y=1) \\
    \end{split}
\end{equation*}

As $D$ is realizable w.r.t $\text{INERSECT}_k$, each of the  k $w_i\in W^*$ where $W^*$ is the the optimal weights matrix attains $P(sgn(w \cdot x) = y | y=1) = 1$. 

Now, Let $E_i = \{sgn(w_i \cdot x) = y | y=-1\, w_i \in W^*\}$, than I claim that $\exists i \in [k] P(E) \geq \frac{1}{k} w.p. 1$. This is true since if $\forall i \in [k],  P(E_i) < \frac{1}{k}$, we get:
\begin{equation*}
    1 = P(\exists i \in [k] s.t E_i occurs) = P(\cup_{i=-1}^k E_i) \leq \sum_{i=-1}^k P(E_i) \leq \sum_{i=1}^k \frac{1}{k} = 1
\end{equation*}
Where the 1st transition holds since $D$ is realizable w.r.t $\text{INERSECT}_k$, thus $E_i$ must occur for some $i\in [k]$ w.p. -1. 

Plugging it all back in:

\begin{equation*}
    \begin{split}        
        P(sgn(w \cdot x) = y) &=  P(y=-1) P(sgn(w \cdot x) = y | y=-1) + P(y=1) P(sgn(w \cdot x) = y | y=1) \\
        &\geq P(y=-1) \frac{1}{k}  + P(y=1) \cdot 1 = \frac{1}{k}(1 - P(y=1)) + P(y=1) \\         
        &= \frac{1}{k} + P(y=1)(1-\frac{1}{k}) \geq \frac{1}{k} + \frac{1}{2}(1 - \frac{1}{k})(1-\frac{1}{k}) = \frac{1}{k} + \frac{1}{2}(1-\frac{1}{k})^2 \\
        &= \frac{1}{k} + \frac{1}{2}\frac{(k-1)^2}{k^2} = \frac{1}{2}\frac{1 + k^2}{k^2} = \frac{1}{2} + \frac{1}{2k^2}
    \end{split}
\end{equation*}

\subsubsection{}
By section (a), we conclude that in any case, the oracle $\mathcal{O}$ returns a $\frac{1}{2k^2}$-weak learner for $INTERSECT_k$ (either by returning $h_t(x_i) = -1$ in the event $P(y=-1) > \frac{1}{2} + \frac{1}{2k}$ or by finding the halfspace that satisfies condition 2). Using AdaBoost and a sufficiently large number of iterations T, we can obtain a strong learning algorithm for $INTERSECT_k$ with sample complexity
\begin{equation*}    
    m(\epsilon, \delta) = O(\frac{Tdlog(Td) + \frac{1}{\delta}}{\epsilon}) = O(\frac{\frac{log m}{\gamma(k)^2}dlog(\frac{log m}{\gamma(k)^2}d) + \frac{1}{\delta}}{\epsilon})
\end{equation*}
Where $\gamma(k) = \frac{1}{2k^2}$. Following Lemma 3.8 taught in class, we get:
\begin{equation*}    
    m(\epsilon, \delta) = O(\frac{\frac{d}{\gamma(k)^2} log (\frac{d}{\gamma(k)^2}) log(\frac{d}{\gamma(k)^2} log (\frac{d}{\gamma(k)^2})) + \frac{1}{\delta}}{\epsilon})
\end{equation*}

