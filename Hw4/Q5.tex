\subsection*{a}
\subsubsection*{1}
\begin{proof}    
    If we plug in $\boldsymbol{w}=\boldsymbol{0}$, we get:
    \begin{equation*}
        \mathcal{L}_{\lambda, S}(\boldsymbol{w=0}) = \frac{\lambda}{2}||\boldsymbol{0}|| 
        + \frac{1}{m} \sum_{i=1}^{m} max\{0, 1-0\} = \frac{1}{m} \sum_{i=1}^{m} 1 = 1
    \end{equation*}
    
    Now let $\boldsymbol{w}_{\lambda, S} = \underset{\boldsymbol{w}}{argmin} \mathcal{L}_{\lambda, S}(\boldsymbol{w})$, so by construction, $\mathcal{L}_{\lambda, S}(\boldsymbol{w}_{\lambda, S}) \leq \mathcal{L}_{\lambda, S}(\boldsymbol{0}) = 1$, proving the desired inequality
\end{proof}

\subsubsection*{2}
\begin{proof}
    Following subsection (1), we have:
    \begin{equation*}
        \begin{split}            
            & \mathcal{L}_{\lambda, S}(\boldsymbol{w}_{\lambda, S}) = \frac{\lambda}{2}||\boldsymbol{w}_{\lambda, S}||^2 
            + \frac{1}{m} \sum_{i=1}^{m} max\{0, 1-y \boldsymbol{w}_{\lambda, S} \cdot \boldsymbol{x}\} \leq 1 \\
            \Rightarrow & \frac{\lambda}{2}||\boldsymbol{w}_{\lambda, S}||^2 \leq 1 - \frac{1}{m} \sum_{i=1}^{m} max\{0, 1-y \boldsymbol{w}_{\lambda, S} \cdot \boldsymbol{x}\} \leq 1
        \end{split}
    \end{equation*}
    Where the last transition is since $max\{0, 1-y \boldsymbol{w}_{\lambda, S} \cdot \boldsymbol{x}\} \geq 0 \; \forall y, \boldsymbol{w}_{\lambda, S}, \boldsymbol{x}$. The desired result is obtained by multiplying both sides by $\frac{2}{\lambda}$ (which doesn't alter the inequality's direction since $\lambda > 0$) and taking the square root (which also doesn't change the inequality's direction since it's a monotonically increasing function).
\end{proof}

\subsection*{b}
\begin{proof}
    There are 2 possibilities:
    \begin{enumerate}
        \item $sign(\boldsymbol{w}\cdot \boldsymbol{x}) = y$, in which case $\ell_{0, 1}(sign(\boldsymbol{w}\cdot \boldsymbol{x})) = 0$ and $\ell_{\text{hinge}}(\boldsymbol{w}; \boldsymbol{x}, y) = max\{0, 1-\alpha\}$ where $\alpha > 0$. Hence, $\ell_{\text{hinge}}(\boldsymbol{w}; \boldsymbol{x}, y)$ can only take values greater then 0, and so is an upper-bound for $\ell_{0, 1}(sign(\boldsymbol{w}\cdot \boldsymbol{x}), y) = 0$ in this case.
        \item $sign(\boldsymbol{w}\cdot \boldsymbol{x}) \neq y$, in which case $\ell_{0, 1}(sign(\boldsymbol{w}\cdot \boldsymbol{x})) = 1$ and $\ell_{\text{hinge}}(\boldsymbol{w}; \boldsymbol{x}, y) = max\{0, 1 + \alpha\} = 1 + \alpha$ where $\alpha > 0$. Hence, $\ell_{\text{hinge}}(\boldsymbol{w}; \boldsymbol{x}, y)$ can only take values greater then 1, and so is an upper-bound for $\ell_{0, 1}(sign(\boldsymbol{w}\cdot \boldsymbol{x}), y) = 1$ in this case.
    \end{enumerate}
    Concluding that $\ell_{0, 1}(sign(\boldsymbol{w}\cdot \boldsymbol{x}), y) \leq \ell_{\text{hinge}}(\boldsymbol{w}; \boldsymbol{x}, y) \; \forall \boldsymbol{w}$. Clearly, since this holds $\forall y, \boldsymbol{x}$, this is also true in expectation, that is:
    \begin{equation*}
        \mathbb{E} \ell_{0, 1}(sign(\boldsymbol{w}\cdot \boldsymbol{x}), y) \leq \mathbb{E} \ell_{\text{hinge}}(\boldsymbol{w}; \boldsymbol{x}, y)
    \end{equation*}

    Simplifying the expectation for the 0-1 loss, we get:
    \begin{equation*}
        \begin{split}            
            \mathbb{E} \ell_{0, 1}(sign(\boldsymbol{w}\cdot \boldsymbol{x}), y)
            &= \ell_{0, 1}(sign(\boldsymbol{w}\cdot \boldsymbol{x}) \neq y) \cdot D(sign(\boldsymbol{w}\cdot \boldsymbol{x}) \neq y) + \ell_{0, 1}(sign(\boldsymbol{w}\cdot \boldsymbol{x}) = y) \cdot D(sign(\boldsymbol{w}\cdot \boldsymbol{x}) = y) \\
            &= 1 \cdot D(sign(\boldsymbol{w}\cdot \boldsymbol{x}) \neq y) + 0 \cdot D(sign(\boldsymbol{w}\cdot \boldsymbol{x}) = y) = D(sign(\boldsymbol{w}\cdot \boldsymbol{x}) \neq y) 
            = err(\boldsymbol{w})
        \end{split}
    \end{equation*}

    Hence:
    \begin{equation*}
        err(\boldsymbol{w}) \leq \mathbb{E} \ell_{\text{hinge}}(\boldsymbol{w}; \boldsymbol{x}, y)
    \end{equation*}
    As required.
\end{proof}

\subsection*{c}
\begin{proof}
    First, we claim that $\forall \boldsymbol{w} \in \mathbb{R}^d$, $\mathcal{L}_{\lambda, S}(\boldsymbol{w}_{\lambda, S}) \leq \mathcal{L}_{\lambda, S}(\boldsymbol{w})$ by the definition of $\boldsymbol{w}_{\lambda, S}$. Hence, it's also true for $\boldsymbol{w}^* = \underset{||\boldsymbol{w}||\leq 1}{argmin} \mathcal{L}_{\lambda, S}(\boldsymbol{w}_{\lambda, S})$. we thus conclude that:

    \begin{equation*}
        \begin{split}            
            \frac{1}{m} \sum_{i=1}^m \ell_{\text{hinge}}(\boldsymbol{w}_{\lambda, S}; \boldsymbol{x}, y) 
            &\leq \mathcal{L}_{\lambda, S}(\boldsymbol{w}_{\lambda, S}) 
            \leq \mathcal{L}_{\lambda, S}(\boldsymbol{w}^*) \\
            &= \underset{||\boldsymbol{w}||\leq 1}{min} \frac{\lambda}{2} ||\boldsymbol{w}||^2 + \sum_{i=1}^m \ell_{\text{hinge}}(\boldsymbol{w}; \boldsymbol{x}, y) \\
            &\leq \frac{\lambda}{2} + \underset{||\boldsymbol{w}||\leq 1}{min} \sum_{i=1}^m \ell_{\text{hinge}}(\boldsymbol{w}; \boldsymbol{x}, y)
        \end{split}
    \end{equation*}
    Where the last transition holds since $||\boldsymbol{w}^*|| \leq 1$ by the section's assumption. Using this result, we can say that:
    \begin{equation*}
        \begin{split}            
            \mathcal{L}(\boldsymbol{w}_{\lambda, S}) - \left(\frac{\lambda}{2} + \underset{||\boldsymbol{w}||\leq 1}{min} \sum_{i=1}^m \ell_{\text{hinge}}(\boldsymbol{w}; \boldsymbol{x}, y)\right) 
            &\leq \mathcal{L}_D(\boldsymbol{w}_{\lambda, S}) - \frac{1}{m} \sum_{i=1}^m \ell_{\text{hinge}}(\boldsymbol{w}_{\lambda, S}; \boldsymbol{x}, y) \\
            &= \mathcal{L}_D(\boldsymbol{w}_{\lambda, S}) - \mathcal{L}_S(\boldsymbol{w}_{\lambda, S})
        \end{split}
    \end{equation*}

    Moving forward, we restrict the domain of $\boldsymbol{w}$ to the ball of radius $B=\sqrt{\frac{2}{\lambda}}$. now, let's find the supremum and Liphschitzness of $\ell_{\text{hinge}}(\boldsymbol{w};(\boldsymbol{x}, y))$|:
    \begin{equation*}
        |\ell_{\text{hinge}}(\boldsymbol{w};(\boldsymbol{x}, y))| \leq |1-y\boldsymbol{w}\cdot\boldsymbol{x}| \leq |1 + \boldsymbol{w}\cdot \frac{\boldsymbol{w}}{||\boldsymbol{w}||}| = 1 + ||\boldsymbol{w}|| \leq 1+\sqrt{\frac{2}{\lambda}} = O(\frac{1}{\sqrt{\lambda}}) \triangleq c
    \end{equation*}
    Where the 2nd last transition assumes $\lambda \leq 1$ (which we'll require in the next sections).
    \begin{equation*}
        |\nabla \ell_{\text{hinge}}(\boldsymbol{w};(\boldsymbol{x}, y))| \leq |\nabla (1-y\boldsymbol{w}\cdot\boldsymbol{x})| = |-y\boldsymbol{x}| \leq ||\boldsymbol{x}|| \leq 1 \triangleq \rho
    \end{equation*}
    Following the proven theorem from section 4, we have:
    \begin{equation*}
        \underset{||\boldsymbol{w}|| \leq B}{sup} \mathcal{L}_D(\boldsymbol{w}) - \mathcal{L}_S(\boldsymbol{w}) \leq 2 \sqrt{\frac{2}{\lambda m}} + O\left(\sqrt{\frac{ln 1/\delta}{\lambda m}} \right)
    \end{equation*}

    Now, combining everything so far, we have:
    \begin{equation*}
        \begin{split}      
            \mathcal{L}(\boldsymbol{w}_{\lambda, S}) - \left(\frac{\lambda}{2} + \underset{||\boldsymbol{w}||\leq 1}{min} \sum_{i=1}^m \ell_{\text{hinge}}(\boldsymbol{w}; \boldsymbol{x}, y)\right)
            &\leq \mathcal{L}_D(\boldsymbol{w}_{\lambda, S}) - \mathcal{L}_S(\boldsymbol{w}_{\lambda, S}) \\
            &\leq \underset{||\boldsymbol{w}|| \leq B}{sup} \mathcal{L}_D(\boldsymbol{w}) - \mathcal{L}_S(\boldsymbol{w}) \leq 2 \sqrt{\frac{2}{\lambda m}} + O\left(\sqrt{\frac{ln 1/\delta}{\lambda m}} \right)
        \end{split}
    \end{equation*}

    And therefore:
    \begin{equation*}
        \begin{split}      
            \mathcal{L}(\boldsymbol{w}_{\lambda, S}) &\leq \frac{\lambda}{2} + \underset{||\boldsymbol{w}||\leq 1}{min} \sum_{i=1}^m \ell_{\text{hinge}}(\boldsymbol{w}; \boldsymbol{x}, y) + 2 \sqrt{\frac{2}{\lambda m}} + O\left(\sqrt{\frac{ln 1/\delta}{\lambda m}} \right) \\
            &= \underset{||\boldsymbol{w}||\leq 1}{min} \sum_{i=1}^m \ell_{\text{hinge}}(\boldsymbol{w}; \boldsymbol{x}, y) + O\left(\lambda + \left(1 + \sqrt{ln 1/\delta}\right)\sqrt{\frac{1}{\lambda m}} \right) \\
            &= \underset{||\boldsymbol{w}||\leq 1}{min} \sum_{i=1}^m \ell_{\text{hinge}}(\boldsymbol{w}; \boldsymbol{x}, y) + O\left(\lambda + \sqrt{\frac{ln 1/\delta}{\lambda m}} \right)
        \end{split}
    \end{equation*}
    Where the last transition holds since typically $\delta << 1$.

\subsection*{d}
If a such a $||\boldsymbol{w*}||$ exists, then $\forall x_i, y_i \in D, \; \ell_{\text{hinge}}(\boldsymbol{w};(\boldsymbol{x}_i, y_i)) = 0 $. As this is the minimum of the empirical loss, the result from subsection c collapses to:

\begin{equation*}
    \mathcal{L}_D(\boldsymbol{w_{\lambda, S}}) \leq O\left(\sqrt{\frac{ln 1/\delta}{\lambda m}} + \lambda \right)
\end{equation*}

Based on the result from subsection b, this also means that:
\begin{equation*}
    err(\boldsymbol{w_{\lambda, S}}) \leq O\left(\sqrt{\frac{ln 1/\delta}{\lambda m}} + \lambda \right)
\end{equation*}
Now, we damand that the RHS is less or equal $\epsilon$ by minimizing the O term.
We note that:
\begin{equation*}
    \begin{split}
        \frac{d}{d\lambda} \left(\sqrt{\frac{ln 1/\delta}{\lambda m}} + \lambda\right) &= -\frac{1}{2} (\frac{ln 1/\delta}{\lambda m})^{-\frac{3}{2}} + 1 \\
        \frac{d^2}{d\lambda^2} \left(\sqrt{\frac{ln 1/\delta}{\lambda m}} + \lambda\right) &= 
        \frac{3}{4} (\frac{ln 1/\delta}{\lambda m})^{-\frac{5}{2}} > 0 \; \forall \epsilon, \delta > 0, \lambda \geq 0
    \end{split}
\end{equation*}
Hence, the function $f(m, \lambda) = \sqrt{\frac{ln 1/\delta}{\lambda m}} + \lambda$ is convex $\forall \lambda \geq 0$, and since $f(\lambda=0), f(\lambda \rightarrow \infty) \rightarrow \infty$, it's extremum is the global minimizer:

\begin{equation*}
    \begin{split}        
        \frac{d}{d\lambda} \left(\sqrt{\frac{ln 1/\delta}{\lambda m}} + \lambda\right) &= -\frac{1}{2} (\frac{ln 1/\delta}{\lambda m})^{-\frac{3}{2}} + 1 \overset{!}{=} 0 \\
        \lambda^* &= \sqrt[3]{\frac{ln 1/\delta}{4\lambda m}} = O(\sqrt[3]{\frac{ln 1/\delta}{\lambda m}})
    \end{split}
\end{equation*}
Plugging $\lambda^*$ and by denoting $a(m) = \frac{ln 1/\delta}{m}$, we have:
\begin{equation*}
    f(m, \lambda^*) = a(m)^\frac{1}{3} + a(m)^\frac{1}{2} a(m)^{-\frac{1}{6}} = 2a(m)^\frac{1}{3} = O(a(m)^\frac{1}{3}) = O(\sqrt[3]{\frac{ln 1/\delta}{\lambda m}})
\end{equation*}
Hence:
\begin{equation*}
    err(\boldsymbol{w_{\lambda, S}}) \leq O(\sqrt[3]{\frac{ln 1/\delta}{m}}) \overset{!}{=} O(\epsilon)
\end{equation*}
Thus by selecting $m = O\left(\frac{ln 1/\delta}{\epsilon^3}\right)$, and thus $\lambda* = O(\epsilon)$, we get the desired result.