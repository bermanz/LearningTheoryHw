Let $I = [l, u]$, the true interval from which the '+' examples come.
First, we note that if $D(I) < \epsilon$, we can w.l.o.g return the all negative hypothesis, which will have an error less than $\epsilon$.
Otherwise, we can pick $\hat{l} > l$ and $\hat{u} < u$ such that $\hat{I}_m = [\hat{l}, \hat{u}]$ is the interval estimated by the algorithm given the sample $S$ of size $m$.
To obtain $\hat{l}, \hat{u}$, we can use:
\begin{equation*}
    \begin{split}
        \hat{l} &= \underset{i\in[m]}{min} x_i \\
        \hat{u} &= \underset{i\in[m]}{max} x_i
    \end{split}
\end{equation*}
which is an ERM algorithm, as the returned interval $\hat{I}_m$ classifies all the sample correctly by design.
To analyse the error obtained by this algorithm, let's define $\tilde{l} > l, \tilde{u} < u$ such that for a random sample $x_i \sim D$, $P(x_i \in [l, \tilde{l}]) = P(x_i \in [\tilde{u}, u]) = \frac{\epsilon}{2}$. In addition, let's define the event $E = \{\exists i,j \in [m] S.T x_i \in [l, \tilde{l}] \cap x_j \in [\tilde{u}, u]\}$.
For auxiliary, we calculate the probability of the complement event, that is:
$\bar{E} = \{\forall i \in [m] S.T x_i \notin [l, \tilde{l}] \cup x_i \notin [\tilde{u}, u]\}$.
Should $\bar{E}$ occur, we can't guarantee a success of our hypothesis. Let's calculate the probability of the event's occurrence:
\begin{equation*}
    \begin{split}        
        P(\bar{E}) &\overset{\text{UB}}{\leq} P(x_i \notin [l, \tilde{l}] \forall i \in [m]) + P(x_i [\tilde{u}, u] \forall i \in [m]) \\
        &= 2 P(x_i \notin [l, \tilde{l}] \forall i \in [m]) = 2(1- \frac{\epsilon}{2})^2 \leq 2e^{-\frac{\epsilon}{2}m}
    \end{split}
\end{equation*}
Now, instead of lower bounding the probability for success, let's upper-bound the probability for failure:
\begin{equation*}
    \begin{split}
        P(err > \epsilon) &\leq P(\bar{E}) \leq 2e^{-\frac{\epsilon}{2}m} \overset{!}{<} \delta \\
        m(\epsilon, \delta) &> \frac{2}{\epsilon} ln(\frac{\delta}{2})
    \end{split}
\end{equation*}