\begin{algorithm}
    \caption{K-Intevral-Union Classifier}\label{alg:cap}
    \begin{algorithmic}
        \Require k, $S\{(x_1, y_1), (x_2, y_2), ..., (x_m, y_m)\}$, $(x_i, y_i) \in \mathbb{R} \times \mathbb{R} \;  \forall i \in [1, m]$
        \State $k_0 \gets 0$
        \For{$i=1:m$}
            \State $l_{nn} \gets \underset{l_j}{argmin} |l_j - x_i|, \; j\in[1, k_0]$
            \State $u_{nn} \gets \underset{l_j}{argmin} |u_j - x_i|, \; j\in[1, k_0]$
            \If{$y_i=1$}
                \If {$k_0 == 0$}
                    \State $l_1 \gets x_i$
                    \State $u_1 \gets x_i$
                \Else
                    \If{$x_i < l_{nn}$}
                        \State $l_{nn} \gets x_i$
                    \EndIf
                    \If{$x_i > u_{nn}$}
                        \State $u_{nn} \gets x_i$
                    \EndIf
                \EndIf
            \Else
                \If{$k_0 > 0$ and $l_{nn} < x_i < u_{nn}$} \Comment{Interval should be split into 2}
                    \State $k_0 \gets k_0 + 1$
                    \State $x_nn_l \gets \underset{x_m}{argmin} |x_m - x_i| s.t x_m < x_i, \; m\in[1, i]$
                    \State $x_nn_u \gets \underset{x_m}{argmin} |x_m - x_i| s.t x_m > x_i, \; m\in[1, i]$
                    \State $u_{k_0} \gets x_nn_l$
                    \State $l_{k_0} \gets x_nn_u$
                \EndIf
            \EndIf
        \EndFor \\
    \Return $\hat{l}, \hat{u}$
    \end{algorithmic}
\end{algorithm}

Following the same logic of excercise 1, We define $I_j = [l_j, u_j] \;, j\in [1, k]$, and $(\hat{I}_m)_j = [\hat{l}, \hat{u}]$.
In contrast to excercise 1, the defined sub-intervals for each j interval are now designed such that $D(\Delta l_j) = D(\Delta u_j) < \frac{\epsilon}{2k}$
(for which inherently $D(\Delta(I_j, \hat{I_j})) < \frac{\epsilon}{k}$). Since D is uniform, the probabilit for a sample not to fall within as $D(\Delta(I_j, \hat{I_j}))$ is the same $\forall j\in[1, k]$:
\begin{equation*}
    P(\underset{i=1:m}{\cup} x_i \notin \Delta(I_j, \hat{I}_j)) = 2\prod_{i=1}^{m}P(x_i \notin \Delta l_j) 
    = 2\prod_{i=1}^{m} 1-\frac{\epsilon}{2k}= 2(1-\frac{\epsilon}{2k})^m \leq 2e^{-\frac{\epsilon}{2k}m}
\end{equation*}


To make the error rate of the complete classifier greater than $\epsilon$, it's necessary that non of the training examples to fall inside
$\Delta(I_j, \hat{I}_j), \; \forall j\in[1,m]$, that is:
\begin{equation*}
    P(err > \epsilon) = P\left(\cup_{j=1}^{k} \left[ \cup_{i=1}^{m} x_i \notin \Delta(I_j, \hat{I}_j)) \right]\right) = \sum_{j=1}^{k} P\left(\cup_{i=1}^{m} x_i \notin \Delta(I_j, \hat{I}_j))\right)
\end{equation*}
Where the last transition is true since the intervals are non-overlapping, making the probabilities independent of eachother.
Thus:
\begin{equation*}
    P(err > \epsilon) \leq \sum_{j=1}^{k} 2e^{-\frac{\epsilon}{2k}m} = 2ke^{-\frac{\epsilon}{2k}m} \overset{!}{<} \delta
\end{equation*}
By taking the log of both sides and simplifying, we get the desired sample complexity:
\begin{equation*}
    m(\epsilon, \delta) > \frac{2k}{\epsilon} log(\frac{2k}{\delta})
\end{equation*}


